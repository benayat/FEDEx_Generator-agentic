## PD-EXPLAIN-AGENTIC
Integration of agentic-llm based explanations to The original fedex-generator based repo for generating explanations to dataframes.

#### The main components:
- Matching the project architecture, I added the `agents` python package, containing the base-agent and the different inheriting classes for the agents that can be used to generate explanations:
* BaseAgent: Base class. Contains the main methods for generating content and communicating with the openai api.
* ExplanationAgent: The agent that generates natural language explanations, based on all the data gathered so far by the fedex-generator.
* CodeInstructorAgent: Based on the explanation and the data, this agent generates self-sufficient coding instructions for the coding agent.
* MatplotlibCodeAgent: Based solely on the code-instructions generated by the CodeInstructorAgent, this agent generates the python/matplotlib code that will be used to generate the visualizations.
* MatplotlibRefineAgent: Takes the generated code by the MatplotlibCodeAgent and refines it to make it more visually appealing.
- agents->AgentsManager: The class that manages the entire pipeline internally. It runs the agents in parallel for each k, while maintaining a sequential process inside each pipeline.

#### How does it work?
- The fedex system is working fine and generating valuable insights, so instead of starting from scratch, I have used the insights from fedex after the algorithm was run, and based the agents work on top of it.
- The `entry point` to the system is in BaseMeasure.py from fedex-generator, in line 232 - where all the relevant data is already gathered, just before the system generates the visualization.
- I used the gathered data as input for the ExplanationAgent, which starts the above-mentioned pipeline of generating explanations, coding instructions and visualizations.

#### Some of the challenges I dealt with:
- The first important challenge was to understand the pd-explain system enough to know that the starting point is actually in the fedex-generator. 
- Llm inference is expensive, so I started with local LLMs with [ollama](https://ollama.com/) in compatibility mode with openai-api, and after the basic architecture was set, I moved to the openai api. 
- LLMs can be slow as well, so for simpler missions like rephrasing the fedex-generated textual explanation, I used a cheaper and faster model like `gpt-4o-mini`, and for more complex missions like generating code, I used the `gpt-4o` model. This way I got the best of both worlds - speed and quality.
- LLMs hallucinate a lot, and it's more noticeable in the code generation since it requires punctuality and correctness. I had to go prompt engineer the instructions for each agent, especially the CodeInstructorAgent, to get the best results.
To get better code results, I used a lower value for the `temperature` parameter in the openai api, to get more deterministic results. I also used a refining agent to go over the code and make it more visually appealing.
- Time. Any way I see it, and no matter what I did, integrating LLMs into the system slowed it down significantly. To mitigate this, I used the `asyncio` library to run the pipelines in parallel for each k. This change drastically improved the running time, but still, the system is slower than the original fedex-generator.


#### Problems with initial implementation, and how I solved them:
- At first, I generated a lengthy paragraph of explanation, which used a lot of output tokens from the api, and eventually I had nowhere to go put it on the screen. To make it simpler and cheaper, I changed the explanation_agent instruction to rephrase the short explanation generated by the fedex-generator to a more simple and easy-to-understand explanation, and used it instead as the plot title. This way, the explanation is more concise and the generation process is faster and cheaper.
-  and executed the code separately for each K. The executor had to be called multiple times, which slowed down the process significantly.
- The code to run the agents in BasicMeasure.py->calc_influence function was working fine eventually, but the code readability was compromised after adding the agents code. To mitigate this, had to make an architectural decision to move out the agents code to the new 'AgentsManager' class, which managed the entire pipline internally, instead of cluttering the BasicMeasure.py file. 
- Instead of the lengthy paragraph for the generated explanation, I changed the explanation_agent instruction to rephrase the short explanation generated by the fedex-generator to a more simple and easy-to-understand explanation, and used it instead as the plot title. This way, the explanation is more concise and the generation process is faster and cheaper.
- Still, the pipeline was way too slow and compromised the user experience. To mitigate this, I used the `asyncio` library to run the agents in parallel for each k, added an agent to combine all codes for one, and run them together. This change drastically improved the running time.

#### Notes and limitations:
- The system is still slower than the original fedex-generator, but the insights are more detailed, coherent and visually appealing.
- From my analysis of the fedex-system [paper](https://www.vldb.org/pvldb/vol15/p3854-gilad.pdf), while very efficient and better than automated competition, a noticeable downside of the fedex-system in comparison to manual EXPERT explanation is the explanation simplicity and coherency, and I quote: `the Expert baseline got higher coherency
scores with an average of 6.3 compared to an average of 5.4 for
fedex`.(arXiv:2209.06260 [cs.DB], 3864). For that purpose, I focused on making the textual explanation more coherent and easy to understand, and the secondary purpose was to make visualizations more visually appealing.
- for the above reason, in my opinion, using the original system as is, and only using the llm agent to make the textual explanation more coherent and easy to understand, could by itself make the fedex-system much closer to the EXPERT baseline. 
- My explanation agent system will not work for the OutlierMeasure, which for some reason and unlike the other measures, doesn't call the `calc_influence` function, which is the entry point for the agents pipeline. Instead, the visualization is generated internally in the OutlierMeasure->explain_outlier function. 
- The system is only intended to show POC of the integration of the agentic-llm based explanations to the fedex-generator, and not for production use. The system is still slow, buggy, and it sometimes draws empty plot before the real one, but it shows the potential of the system to generate detailed and coherent explanations for the fedex insights.


#### summary of code changes from original fedex-generator and pd-explain:
- Added the agents package.
- Since the update for async api pipelines, I had to bubble up `async` def and `await` calls. The changes are in the following places: in pd-explain-> [explainable_data_frame,explainable_group_by_dataframe, explainable_series] files -> explain func,
fedex-generator->operations->[Filter,GroupBy,Join]->explain, and Measures->BasicMeasure->calc_influence.
#### usage-without docker:
- Clone the repo and create a venv virtual environment with python 3.12.
- Run `pip install -e .` in the root directory.
- Clone the updated pd-explain [repo](https://github.com/benayat/pd-explain-agentic.git)
- Run `cd pd-explain-agentic && pip install -e .`
- Add your openai api key and base url to the `OPENAI_API_KEY` and `OPENAI_BASE_URL` environment variables.
- Run Notebooks/agents_adults-DEMO.ipynb for a demo.
- Note: The notebook is currently set to load environment variables from a .env file, so make sure to add the api key and base url to the .env file, or change the notebook to load the variables from the environment.
